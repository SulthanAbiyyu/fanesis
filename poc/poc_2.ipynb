{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  C:\\Users\\user /.deepface created\n",
      "Directory  C:\\Users\\user /.deepface/weights created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verified': True,\n",
       " 'distance': 0.2204041458955489,\n",
       " 'threshold': 0.4,\n",
       " 'model': 'Facenet',\n",
       " 'detector_backend': 'opencv',\n",
       " 'similarity_metric': 'cosine',\n",
       " 'facial_areas': {'img1': {'x': 259, 'y': 87, 'w': 126, 'h': 126},\n",
       "  'img2': {'x': 188, 'y': 38, 'w': 128, 'h': 128}},\n",
       " 'time': 0.4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeepFace.verify(\"../data/img1.jpg\", \"../data/img8.jpg\", model_name=\"Facenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hasil = DeepFace.extract_faces( \"../data/img3.jpg\")\n",
    "plt.imshow(hasil[0][\"face\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hasil[1][\"face\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_1 = hasil[1][\"face\"]\n",
    "\n",
    "result = DeepFace.analyze(\"../data/img3.jpg\", actions=(\"emotion\", \"gender\"))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(5, 10))\n",
    "ax[0].imshow(hasil[0][\"face\"])\n",
    "ax[1].imshow(hasil[1][\"face\"])\n",
    "ax[0].set_title(f\"{result[0]['dominant_emotion']} | {result[0]['dominant_gender']}\")\n",
    "ax[1].set_title(f\"{result[1]['dominant_emotion']} | {result[1]['dominant_gender']}\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8990562295931955"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cos_sim(a, b):\n",
    "    a = np.array(a[0][\"embedding\"])\n",
    "    b = np.array(b[0][\"embedding\"])\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "person_1 = hasil[1][\"face\"]\n",
    "person_1_juga = DeepFace.extract_faces( \"../data/img1.jpg\")\n",
    "\n",
    "# nggak pake backend karena udah di crop -> untuk meyakinkan kalo itu wajah yang dimaksud\n",
    "embed_1 = DeepFace.represent(person_1, model_name=\"Facenet\", detector_backend=\"skip\")\n",
    "embed_2 = DeepFace.represent(person_1_juga[0][\"face\"], model_name=\"Facenet\", detector_backend=\"skip\")\n",
    "\n",
    "cos_sim(embed_1, embed_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
